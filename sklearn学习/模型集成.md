集成学习

Bagging：随机森林
Boosting：Adaboost, GBDT, XGBoost, LightGBM, CatBoost
Stacking: Stacking
Blending: Blending

集成的基础技术：
1. 最大投票法
2. 平均法
3. 加权平均法

高级集成技术：
1. stacking
2. blending

示例代码：
我们首先定义一个函数来对n折
```python
def Stacking(model, train, y, test, n_fold):
    folds = StratifiedKFold(n_splits=n_fold, random_state=1, shuffle=True)
    test_pred = np.empty((test.shape[0], 1), float)
    train_pred = np.empty((0, 1), float)#??

    for train_indices, val_indices in folds.split(train, y.values):
        x_train, x_val = train.iloc[train_indices], train.iloc[val_indices]
        y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]

        model.fit(X=x_train, y=y_train)
        #拿train来得到下一层的输入
        train_pred = np.append(train_pred, model.predict(x_val))
        test_pred = np.append(test_pred, model.predict(test))
        #test没做平均吗？

    return test_pred.reshape(-1, 1), train_pred
```
```python
model1 = tree.DecisionTreeClassifier(random_state=1)

test_pred1, train_pred1 = Stacking(model=model1, n_fold=10, 
train=x_train, test=x_test, y=y_train)

train_pred1 = pd.DataFrame(train_pred1)
test_pred1 = pd.DataFrame(test_pred1)

model2 = KNeighborsClassifier()
test_pred2, train_pred2 = Stacking(model=model2, n_fold=10, train=x_train, test=x_test, y=y_train)

train_pred2 = pd.DataFrame(train_pred2)
test_pred2 = pd.DataFrame(test_pred2)
```
创建第三个模型，逻辑回归，在决策树和knn模型的预测之上
```python
df = pd.concat([train_pred1, train_pred2], axis=1)
df_test = pd.concat([test_pred1, test_pred2], axis=1)

model = LogisticRegression(random_state=1)
model.fit(df, y_train)
model.score(df_test, y_test)
```

