### 预剪枝，后剪枝？
预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力（在训练时加入验证集随时进行泛化验证）的提升，则停止划分并将当前结点标记为叶节点；后剪枝则是先从训练集中生成一颗完整的树，然后自底向上对非叶节点进行考察，若该节点对应的子树替换为叶节点能够提升泛化能力，则进行剪枝将该子树替换为叶节点，否则不剪枝。

很明显，预剪枝技术抑制了很多分支的展开，这降低过拟合的同时还减少了训练时间，但是却存在欠拟合的风险；预剪枝基于“贪心”策略，往往可以达到局部最优解却不能达到全局最优解，也就是说预剪枝生成的决策树不一定是最佳的决策树。XGBoost和lightGBM使用的树就是预剪枝的CART决策树，这能保证他们的训练速度较快，但是准确率如何保证呢？你知道吗，我们大概在讲随机森林的时候再提这点。。

后剪枝技术通常比预剪枝保留了更多的分支，它是自底向上的剪枝，因此它的欠拟合风险较小，泛化能力往往优于预剪枝，然而因为总是要完全生长一棵树，这就要花费很多时间训练了，数据集规模大、维度高时并不适用实际应用。

[这可能是你看过的最用心的【决策树算法】介绍文章](https://zhuanlan.zhihu.com/p/32053821)


### Id3

决策树生成过程：
1. 从根节点开始，计算所有特征的信息增益，选择信息增益最大的特征作为结点特征。
2. 再对子节点递归的调用以上方法，构建决策树。
3. 所有特征信息增益很小或者没有特征可以选择时，递归结束得到一颗决策树。

缺点：
* 对类别较多的属性有偏好，例如某个属性A只有一个可选属性，那么他的信息增益是0
* 无法处理缺失值，直接忽视缺失值
* 无法处理连续值



### C4.5是在Id3基础上改进的，主要采用了信息增益比，连续值处理，缺失值处理
克服了id3的问题？？

### c4.5 缺失值？？



### sklearn中决策树参数：
DecisionTreeClassifier的常用参数含义：
* criterion: 'gini' or 'entropy'(default)，前者是基尼系数，后者是信息熵。
* max_depth: 决策树最大深度。常用来解决过拟合。
* min_impurity_decrese: 这个值限制了决策树的增长，如果某节点的不纯度（基尼系数、信息增益）小于这个阈值，
  则该节点不再生成子节点。
* min_samples_split: 如果是int，则取传入值本身作为最小样本数。如果是float，则用celi(min_samples_split*样本数量)
  的值作为最小样本数，即向上取整。

* min_samples_leaf: 如果是int，则取传入值本身作为最小样本数；如果是float，则取ceil(min_samples_leaf*样本数量)
  的值作为最小样本数，即向上取整。这个值限制了叶子结点最少的样本数，如果某叶子结点数目小于样本数，
  则会和兄弟结点一起被剪枝。

* max_leaf_nodes: 最大叶子节点数。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，
  具体的值可以通过交叉验证得到。





min_samples_split=5,指定分割 内部节点所需的最小样本数，
而 min_samples_leaf指定 叶节点处所需的最小样本数。

例如：
如果min_samples_split=5, 并且内部节点有7个样本，则允许拆分。但是我们说，
分裂产生了两片叶子，一个带有1个样本，另一个带有6个样本。如果 min_sample_leaf=2，
则不允许分割。（即使内部有7个样本，因为生成的叶子之一将少于叶子结点所需的最小样本数）




参考：
1. [决策树汇总](https://zhuanlan.zhihu.com/p/103235259)
2. [为什么C4.5决策树能处理连续特征，ID3树不能处理连续特征？](https://www.zhihu.com/question/425720956)
3. [ID3、c4.5、cart、rf到底是如何处理缺失值的？](https://zhuanlan.zhihu.com/p/84519568)
4. [决策树 ID3 C4.5 cart 总结](https://zhuanlan.zhihu.com/p/86679767)
5. 

