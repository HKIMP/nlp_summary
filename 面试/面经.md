## nlp
说一下Bert嵌入层，Bert相关问题

positional embedding

attention self-attention

### word2vev
word2vec 负采样

word2vec原理，词向量是怎么训练出来的

word2vec和fasttext区别
word2vec和fasttext原理上的区别

Word2vec参数量计算

cbow和skip-gram区别，平时使用哪种方式

介绍一下skip-gram的训练过程，以及使用的加速训练技巧

word2vec有什么优缺点

画一下gru的结构

介绍一下rnn，lstm，transformer各自的优缺点

介绍一下梯度消失，梯度爆炸以及解决方法

rnn为什么容易出现梯度消失、爆炸的原因，能否使用一些技巧缓解

lstm如何解决梯度消失，爆炸的问题

实践中，如何解决梯度爆炸问题，超参数如何设置

设置梯度裁剪阈值时，有没有尝试通过参数分布来计算，而不是直接使用超参数。
### text-cnn, LSTM
介绍一下lstm的原理（h和output的关系）

lstm使用的激活函数是什么？能否使用relu

lstm的参数量，以及初始化方式

有没有见过类似lstm，gru中这种门机制的网络架构

textcnn卷积核大小，为什么要对文本进行卷积，卷积核大小选取标准

textcnn中卷积核的物理意义是什么，提取的是全局特征还是局部特征

textcnn和图像中的cnn最大的区别是什么


### transformer
transformer原理以及自己的理解（相比与之前的方法有哪些优势）

transformer激活函数的位置

transformer的时间复杂度和空间复杂度

transformer的decoder阶段都哪些地方使用了pad

transformer的attention中的softmax中 根号d 代表什么，作用是什么

### bert
讲一讲bert的结构和原理


不考虑多头的原因，self-attention中词向量不乘QKV参数矩阵，会有什么问题？
self-attention的核心是用文本中的其他词来增强目标词的语义表示，从而更好地利用上下文信息。
self-attention中，sequence中的每个词都会和sequence中的每个词做点积去计算相似度。也包括这个词本身

对于self-attention，一般会说它的q=k=v，这里的得相等实际上是指他们来自同一个基础向量，
而在实际计算时，他们是完全不一样的，因为这三者都是乘了QKV参数矩阵的。如果不乘，每个词对应的qkv就是完全一样。

在相同量级的情况下，qi和ki的点积值会是最大的，那在softmax后的加权平均中，该词本身所占的比重将是最大的。
使得其他词

bert为什么要用cls，mlm两种训练方式

bert是如何利用位置信息的（如何训练位置向量），不同方式之间有什么区别

bert相比Word2vec有哪些优势

bert为什么能解决一词多义的问题

bert随机替换掉15%的词，为什么要随机有80%替换为MASK,10%要替换为别的单词
随机10%不替换

DSSM模型和ESIM模型的区别

Bert原理，bert随机mask掉15%的词，为什么要随机有80%替换为[mask],
10%要替换为别的单词，随机10%不替换

文本匹配怎么使用bert

相比于bert得到的各个词向量，直接使用得到的句向量有什么优点

使用bert词向量时，是将文本输入得到output还是直接使用embedding向量，为什么

看过哪些bert改进之后的方法（ALBert，XL-net）

介绍一下ELMo，BERT，GPT之间的区别

self-attention attention multihead-attention mask-attention原理，实现细节

attention如何参考词的位置信息

auc原理（衡量分类器排序的能力），计算（100正，900负，分类器是从0到1均匀分布，计算auc）

样本不平衡如何改进，影响了样本原来的分布该怎么办

代码实现梯度下降

知道哪些模型评价指标

auc和f1的区别，分别使用在什么场景

介绍l1 l2正则化

实现求auc算法，实现lstm

给一个rand5，如何用代码实现rand7
构建[rand5()-1]*5+rand5()
第一项{0, 5, 10, 15, 20},
第二项{1, 2, 3, 4, 5},
加和结果{1, 2, ..., 25}
踢掉大于21的数，剩下的仍是等概率出现的，在对7求余在加1即可得到rand7()

排序算法稳定性，有哪些稳定的排序算法和不稳定的排序算法

了解哪些分类器

了解哪些损失函数

介绍ner，pos

手推logistic regression


自编码

对文本分类最新的方法了解多少？

是否了解label embedding

tfidf原理，公式，弊端是什么


DSSM模型和ESIM模型的区别


## ml
逻辑回归

xgboost

说一下机器学习的pipeline

一般会对原始数据做哪些处理和分析

了解特征工程吗，使用过、知道哪些方法

介绍一个WOE，IV

为什么金融行业使用woe，iv而不是其他特征工程方法

训练集，测试集划分，交叉验证原理

介绍一下训练集，验证集的使用情况，以及实践中遇到的一些问题和技巧。

是否遇到过样本不均衡问题，是如何解决的？

知道哪些解决样本不均衡方法，各自的优缺点，自己实现了哪些，效果如何？

当训练数据和真实数据分布不一样时，线上预测效果会不会出现偏差?

多分类任务中，遇到某几个类别区分不开的情况是怎么处理的？

如何根据训练结果判断什么时候是过拟合什么时候是欠拟合？

过拟合的解决方法，优缺点，自己实现了哪些，效果如何？


知道哪些损失函数，优缺点及适用问题？

写一下CrossEntropy公式，并说明一下他的物理意义

F1值计算公式

介绍一下Precision，Recall

AUC原理，为什么更适用于排序问题

介绍一下信息熵，信息增益，信息增益比
信息熵，信息增益，信息增益比各自偏向于什么样的特征

介绍GBDT

XGBoost，Random Forest区别

Random Forest的随机性体现在哪些方面

XGBoost， GBDT区别

XGBoost的原理，有哪些优点

XGBoost子树每个节点是如何分裂的

XGBoost特征选择方式（空间取值的遍历方式）

XGBoost为什么用CART

XGBoost使用细节（特征维度太高的时候，为什么面临输入问题，如何解决）

调用XGBoost使用的个库

介绍一下LightGBM

LightGBM和XGB有什么差异，带来了哪些改进，如何做到的？

是否了解其他的集成模型，介绍一下

决策树分支的标准，gbdt和xgboost的区别

k-means的k值如何确定

相比于直接使用传统分类器，集成学习的方法有哪些优点？

树模型和其他模型之间最大的区别是什么？

为什么树模型不需要对特征进行标准化处理？

为什么一般预测模型要对特征进行标准化处理？

不进行标准化处理会带来哪些问题？为什么？

说说LR 和 SVM的区别

介绍下SVM

SVM损失函数是什么？编程实现一下？

TSVM半监督算法原理及实现细节

是否了解强化学习？简单介绍一下

如何理解多模态？





## 算法题
输出幂集
给出一个无序数组，输出最小的不在数组中的正数。

## 工程情况

## 推荐系统


## 模型部署
## 鸡汤
不要想着一次性学完，每次学一些，积累，总结。

## 方向
问答，mrc（机器阅读理解），对话，匹配，词向量，
迁移，分类，分词，pos，ner

## 模型篇(mrc,问答，分类，ner)
* SGNS/cbow,fast, elmo 词向量
* DSSM, DecAtt, ESIM 问答&匹配
* HAN，DPCNN，分类
* BIDAF，DrQA，QANet MRC
* Cove，InferSent 迁移
* MM， N-shortest 分词
* BI-lstm, crf NER
* LDA等主题模型 文本表示

## 训练篇
* point-wise, pair-wise, list-wise
* 负采样, nce
* 层级softmax，哈夫曼树构建
* 不均衡问题处理
* KL散度，交叉熵函数

## 评价指标
* F1-score
* PPL
* MRR, MAP

## Python
Python里边哈希表对应哪种结构，是如何解决哈希冲突的
Python深浅拷贝
Python中字典的查找时间复杂度

## 百度实习面经1
### 一轮
1. 先进行自我介绍
2. 介绍自己最熟悉的项目
3. 因为看到你说的这个项目里用到了BERT，那么介绍下Transformer模型
4. Transformer相对于传统的RNN网络有什么好处
5. Transformer里的self-attention作用是什么，有什么优势
6. 你提到了梯度消失的问题，那么Transformer相对于RNN为什么能避免梯度消失
7. 产生梯度消失的原因有哪些
8. 如果你在训练的时候，发现模型不收敛，你会怎么做，可能的原因是什么
9. 学习率是否也会导致模型不收敛
10. 那么学习率应该怎么选择
11. 你提到了优化方法，那么介绍下SGD和ADAM的区别
12. ADAM怎么实现自适应学习率
13. 分类时，样本不均衡问题如何解决
14. 在你的项目中，看到你用CNN，CNN在nlp领域有什么优势和弊端
15. 做题：有序链表合并
16. 最长回文子串
### 二轮
1. 那你就直接介绍一下你最熟悉的一个项目吧
2. 你的项目具体是怎么做的
3. 你的这个数据如何获取，做了什么处理
4. 你的文本大概长度是个什么规模
5. 看你有个文本相似度的项目，你这个项目里用的损失函数是什么
### 三轮
1. 做个自我介绍
2. 介绍一下你最熟悉的项目
3. 你为什么要用你的这个方法
4. 你的这个任务的目标是什么
5. 你对今后从事的工作方向和领域有什么预期，有没有相关的了解或调研？

## 百度健康业务部算法实习
1. tf-idf公式是什么，对于低频词和高频词有处理吗，高频词算出来的tf-idf的值会更大还是更小
2. 特征工程怎么做的
3. xgboost原理
4. w2v原理
5. 为什么会过拟合，怎么解决，归一化的作用
6. 算法题：有序重复数组的查找

## 美团暑期实习NLP工程师面经（技术+HR） 已拿offer
### 一面
1. 简单聊一下工作意向
2. 自我介绍
3. 介绍论文贡献，流程及细节
4. GAN为什么会work
5. vae为什么会work
6. local attention, global attention区别
7. Bert为什么只用Transformer的encoder不用decoder
8. 排队论模型服从什么分布
9. lstm和gru的区别和使用场景
10. 贝叶斯分类的前提假设
11. svm的理论依据，如何推导
12. 编程求2^n

### 二面
1. 介绍文本生成任务
2. 介绍lstm的细节，相关机制，为什么有效
3. 正则化有什么用，为什么有用，L1正则化为什么能使参数稀疏，为什么能防止过拟合。

## 百度暑期实习nlp
### 一面
1. 自我介绍
2. 聊做过的项目，这里注意要按照逻辑顺序交代项目背景，遇到的挑战，手段，成果，
让面试官更清晰的了解你做过的项目。
3. 最大的自序和
4. 树模型，ID3，c4.5区别，RF和GBDT的区别，GBDT是否适合于处理大规模的ID特征
5. 直到哪些激活函数，都有什么特点，如何使用

### 二面
1. 聊做过的项目，和一面讲的是同一个项目。
2. 项目中用过CNN，CNN有什么优势和特点，常用的pooling方法有哪些，哪个更好
3. dssm模型
4. 逻辑回归特征重复了一维会有什么影响

### 三面
1. 聊过做的项目
2. 项目如何验证有效性，在业务中怎么用
3. 聊聊项目中的bad case
4. 训练时，样本不平衡问题如何解决，小样本问题如何解决
5. 给一个任务，垃圾邮件分类，分阶段和流程细致描述一下每一步应该怎么做，以及最终要优化的效果。

### 百度暑期实习
### 一面
1. 查找有序数组的特定元素（测试用例，处理异常？）
2. 实现k进制加法
3. 谈谈LR，SVM的理解
4. 谈谈你了解的优化器
5. 问Adam的特点，总结和Momentum的关联
6. L1,L2的特点
### 二面
1. 问了主语言
2. 文本匹配你知道哪些策略
3. BERT结构和任务，学习目标是什么，怎么设计让他学到语义信息的
4. LSTM，GRU的异同
5. 了解PairWise吗
6. 算法题：编辑距离

## 阿里达摩院
### 一面
1. 研究生时期的研究方向
2. 研究内容有提到DSSM，询问是否知道现在对DSSM的改进模型
3. 询问对生成模型的了解，解码策略，beam search和random sample策略
4. cnn模型中池化层的作用，Max pooling是如何反向传递梯度的
5. 机器学习中正则化是做什么的，约束模型参数，防止过拟合
6. 正则化有L1和L2正则化，区别是什么？扯了一下空间什么的，这一部分参考《百面机器学习》
中的l1[正则化与稀疏性]部分的内容
7. 问深度学习，Transformer模型架构说一下，按照图结构balabala说下
8. Dropout有什么用？类似于Bagging。在Transformer模型中的dropout主要用在哪里呢？
dropout在每个子层之间，设置为0.1.看过源码吗？哈佛实现的那版。
9. Transformer和Bert的位置编码有什么区别
10. Transformer用的Layer Normalize还是Batch Normalize？Layer有什么区别
11. 传统机器学习会哪些，决策树和GBDT区别说一下
12. sigmoid和relu区别，relu解决了什么问题
13. Python垃圾回收机制
### 二面
1. DSSM语义匹配模型及其变种
2. 预训练模型：Transformer，Bert，UniLM等模型的细节，模型中Attention使用，Mask使用。
3. 文本生成任务实际问题：一对多如何训练（从数据角度，模型角度创建一对一条件）
   
## 腾讯
### 一面
1. 谈下做过的项目
2. Dropout了解吗，说下作用，白板编码实现一下？
3. 梯度爆炸，消失了解吗
4. 一对恶意文本case，怎么检测去除，传统方法，AI方法
5. 算法题：最长重复子序列，有序含重复值数组找某个值第一次出现的位置。

### 二面
1. 研究内容，并画出seq2seq结构
2. 天池比赛细节
3. Python列表合并方法有哪些，加法，extend，区别，旧内存如何处理

## 微软
### 一面
1. 问研究方向和实习工作内容
2. Transformer结构，BERT有几种Embedding编码，分词方法
3. 能实现下Word Piece？或者从若干文件中生成一个词典，word2idx, idx2word
4. 能否自己实现sort函数，快排
   
## 美团
### 一面
1. 开放性问题：如何根据美团的商品评论，生成商品的描述。传统抽取方法，语料大后上深度模型。
采用类似于TF-idf的思想避免抽取的描述太大众化没有特点
2. 算法题：top-k 打印N个数组整体最大的top-k
3. bert理解

## 百度一面
1. 比赛的特征工程是怎么做的？
2. 为什么用XGB做融合
3. 1w多条数据，XGB参数怎么调
4. 训练了多少颗树
5. 有没有出现过过拟合出现？
6. 欠拟合和过拟合
7. 解释下为什么数据不做任何处理会导致模型过拟合
8. 讲一下XGB
9. 树模型的剪枝算法，完整剪枝流程
10. L1和L2正则化
11. 讲一下Bagging
12. RF和XGB哪个更深
13. LR里面做归一化，离散化之类的好处
14. LR的损失函数
15. 牛顿法，拟牛顿法
16. LR除了用梯度下降，还可以用牛顿法，矩阵是可逆的（优点）
17. 算法题：数组里面找连续和为s（子数组，是连续的），长度最小的
18. Python多线程，元组，列表实现，元组怎么转列表
19. Python GIL
20. Shell脚本怎么传递参数
21. CNN卷积后feature大小怎么计算，乘法和加法计算了多少次
22. 过拟合，欠拟合
23. 有哪些损失函数
24. 算法题：最大矩形面积

## 美团
### 一面
1. 自我介绍
2. 实习内容，模型参数如何设置，训练多久
3. 概率题。一个单位圆内随机取一点，求到圆心距离的期望。
面试官先要求用代码模拟一下结果，然后再用数学推导结果。
（代码模拟直接随机取坐标(x, y) (x,y in [-1, 1]，圆心为(0,0)，模拟1000词，求均值结果）
（数学推导，求分布函数 -> 密度函数 -> 积分) 答案是2/3
4. 编程题：字符串内有小中大括号和其他字符，判断括号是否匹配，用栈

### 二面
1. 弱人工智能和强人工智能差在哪里
2. 介绍一下Transformer，有什么可以调整的参数
3. 具体讲一下self attention
4. 讲一下attention
5. self attention, attention, 双向lstm的区别
6. 常见的激活函数有哪些。各自有什么特点。分布应用于场景。leaky relu公式。
7. layer norm 和 batch norm区别。各自的应用场景，已经各自在norm之后还有没有其他操作。
训练的时候和测试的时候的区别。
8. 算法题：最长公共子序列。

## 面试
1. rnn真的就梯度消失了吗
2. lstm到底解决了什么，解决了梯度消失？
3. lstm三种门以及sigmoid函数对每个门的作用
4. self-attention的query, key, value分别是什么
5. self-attention的乘法计算和加法计算有什么区别？什么时候加？为什么要除以一个根号？
6. lstm各个模块分别使用什么激活函数，可以使用别的激活函数吗
7. 多头注意力机制的原理是什么
8. Transformer用的是那种attention机制
9. 画一下Transformer的结构图
10. word2vector 如何做负采样，是在全局采样，还是在batch采样？如何实现多batch采样？
怎么确保采样不会采到正样本？word2vec负采样的时候为什么要对频率做3/4次方
11. word2vec经过霍夫曼或者负采样之后，模型与原来的相比，是等价的还是相似的
12. 介绍一个sigmoid和relu，relu有什么缺点
13. 深层神经网络为什么不好训，除了梯度消失还有哪些原因

## 快手
### 一面
1. 自我介绍
2. 简历的第一个项目
3. 项目中用到的模型，最好的模型，为什么最好分析一下
   相比baseline提高了多少，用的什么评价
4. 算法题：五子棋
5. 
transformer并行化更好的一个重要原因，是decoder中，encoder-decoder attention中decoder的输出
可以用teacher-forcing的方式直接得到，因此不用再串行化的去计算decoder的一个一个输出。

相反rnn即使要用teacher-forcing的方式训练，也是要串行实现的。
### 二面
1. 自我介绍
2. 项目1的背景介绍一下
3. 如何构造句子向量的，gensim介绍一下？
4. word2vec原理？skipgram和cbow详细说一下？那个用的多，skipgram为什么在解决罕见词方面表现更好?
   训练中两种优化方式介绍一下？为什么能优化？
5. 介绍一下transformer的结构，encoder的输入，decoder的输入。介绍一下self-attention，为什么比rnn好。
   encoder和decoder都能并行？decoder的mask
6. 介绍一下bert？介绍一下两个预训练任务？介绍一下如何fine-tune？如何在bert后边接网络？
   为什么要比直接输出要好
7. 每个模型的准确率，提升了多少


## 美团
1. 介绍下LR，思想，LR里的最大似然是怎么回事？
2. 代码题：给出一个函数的rand1，随机生成0和1，写一个函数randN，均匀生成0-n-1
3. 代码题：计算根号N，精确到小数点后M位，二分法，梯度下降法
4. 多线程，多进程讲一下，内存占用方式，为什么线程是共享内存的。
5. 有没有碰到过拟合，过拟合的标志是什么，怎么解决
6. 了
7. l1正则化为什么能缓解过拟合
8. 降维方法有哪些，pca是怎么做的，还知道其他的吗
9. lr和神经网络有什么区别
10. 代码题：字符串中长度为len的连续字串中，出现次数最多的字串。1297

## 字节
1. 分类里用到的交叉熵公式是什么
2. pytorch里function和module有什么区别
3. pytorch里dataset，dateloader，sampler有什么区别
4. python里的生成器是什么
5. leetcode4
6. 概率题：在一个圆里，随机取n个点，他们在同一个圆的概率是多少
7. leetcode 53
8. 线程，进程区别
9. lr损失是什么，怎么得到的，为什么不用mse，sifmoid求导是什么
10. 如何防止过拟合
11. bn层作用，dropout的作用
12. l1和l2哪个能使参数分布趋于稀疏，为什么
13. 特征选择的方式
14. 连续特征离散化的好处
15. 偏差和方差的理解
16. 过拟合和欠拟合
17. bn和ln区别
18. 栈空间和堆空间区别
19. bp的过程，cnn中前向传播和后向传播的过程
20. leetcode 138
21. leetcode 543
22. random7 生成 random10 调用次数的期望是多少，怎么算的
23. lru
24. bn层原理，要解决什么问题，还有什么norm方法
25. dropout原理，bn和dropout能不能一起用
26. 代码题：给定一个序列，求其连续子序列和大于某个值得最小长度
27. bn的原理，要解决的问题，公式，训练测试中的区别
28. 如果让模型速度提高一倍，有什么解决方案
29. 代码题：数组中第k大的数，Partation的时间复杂度。
30. mean pooling和max pooling反向传播的不同
31. lstm的反向传播，公式
32. Pytorch中bn层训练和测试有什么不同，怎么实现的。
33. 可分离卷积思想，参数量计算
34. python中类方法和静态方法的区别
35. linux下查看内存占用top，buffers, cache的区别
36. 代码题：买卖股票的最佳时机