## nlp
说一下Bert嵌入层，Bert相关问题

positional embedding

attention self-attention

### word2vev
word2vec 负采样

word2vec原理，词向量是怎么训练出来的

word2vec和fasttext区别
word2vec和fasttext原理上的区别

Word2vec参数量计算

cbow和skip-gram区别，平时使用哪种方式

介绍一下skip-gram的训练过程，以及使用的加速训练技巧

word2vec有什么优缺点

画一下gru的结构

介绍一下rnn，lstm，transformer各自的优缺点

介绍一下梯度消失，梯度爆炸以及解决方法

rnn为什么容易出现梯度消失、爆炸的原因，能否使用一些技巧缓解

lstm如何解决梯度消失，爆炸的问题

实践中，如何解决梯度爆炸问题，超参数如何设置

设置梯度裁剪阈值时，有没有尝试通过参数分布来计算，而不是直接使用超参数。
### text-cnn, LSTM
介绍一下lstm的原理（h和output的关系）

lstm使用的激活函数是什么？能否使用relu

lstm的参数量，以及初始化方式

有没有见过类似lstm，gru中这种门机制的网络架构

textcnn卷积核大小，为什么要对文本进行卷积，卷积核大小选取标准

textcnn中卷积核的物理意义是什么，提取的是全局特征还是局部特征

textcnn和图像中的cnn最大的区别是什么


### transformer
transformer原理以及自己的理解（相比与之前的方法有哪些优势）

transformer激活函数的位置

transformer的时间复杂度和空间复杂度

transformer的decoder阶段都哪些地方使用了pad

transformer的attention中的softmax中 根号d 代表什么，作用是什么

### bert
讲一讲bert的结构和原理


不考虑多头的原因，self-attention中词向量不乘QKV参数矩阵，会有什么问题？
self-attention的核心是用文本中的其他词来增强目标词的语义表示，从而更好地利用上下文信息。
self-attention中，sequence中的每个词都会和sequence中的每个词做点积去计算相似度。也包括这个词本身

对于self-attention，一般会说它的q=k=v，这里的得相等实际上是指他们来自同一个基础向量，
而在实际计算时，他们是完全不一样的，因为这三者都是乘了QKV参数矩阵的。如果不乘，每个词对应的qkv就是完全一样。

在相同量级的情况下，qi和ki的点积值会是最大的，那在softmax后的加权平均中，该词本身所占的比重将是最大的。
使得其他词

bert为什么要用cls，mlm两种训练方式

bert是如何利用位置信息的（如何训练位置向量），不同方式之间有什么区别

bert相比Word2vec有哪些优势

bert为什么能解决一词多义的问题

bert随机替换掉15%的词，为什么要随机有80%替换为MASK,10%要替换为别的单词
随机10%不替换

DSSM模型和ESIM模型的区别

Bert原理，bert随机mask掉15%的词，为什么要随机有80%替换为[mask],
10%要替换为别的单词，随机10%不替换

文本匹配怎么使用bert

相比于bert得到的各个词向量，直接使用得到的句向量有什么优点

使用bert词向量时，是将文本输入得到output还是直接使用embedding向量，为什么

看过哪些bert改进之后的方法（ALBert，XL-net）

介绍一下ELMo，BERT，GPT之间的区别

self-attention attention multihead-attention mask-attention原理，实现细节

attention如何参考词的位置信息

auc原理（衡量分类器排序的能力），计算（100正，900负，分类器是从0到1均匀分布，计算auc）

样本不平衡如何改进，影响了样本原来的分布该怎么办

代码实现梯度下降

知道哪些模型评价指标

auc和f1的区别，分别使用在什么场景

介绍l1 l2正则化

实现求auc算法，实现lstm

给一个rand5，如何用代码实现rand7
构建[rand5()-1]*5+rand5()
第一项{0, 5, 10, 15, 20},
第二项{1, 2, 3, 4, 5},
加和结果{1, 2, ..., 25}
踢掉大于21的数，剩下的仍是等概率出现的，在对7求余在加1即可得到rand7()

排序算法稳定性，有哪些稳定的排序算法和不稳定的排序算法

了解哪些分类器

了解哪些损失函数

介绍ner，pos

手推logistic regression


自编码

对文本分类最新的方法了解多少？

是否了解label embedding

tfidf原理，公式，弊端是什么


DSSM模型和ESIM模型的区别


## ml
逻辑回归

xgboost

说一下机器学习的pipeline

一般会对原始数据做哪些处理和分析

了解特征工程吗，使用过、知道哪些方法

介绍一个WOE，IV

为什么金融行业使用woe，iv而不是其他特征工程方法

训练集，测试集划分，交叉验证原理

介绍一下训练集，验证集的使用情况，以及实践中遇到的一些问题和技巧。

是否遇到过样本不均衡问题，是如何解决的？

知道哪些解决样本不均衡方法，各自的优缺点，自己实现了哪些，效果如何？

当训练数据和真实数据分布不一样时，线上预测效果会不会出现偏差?

多分类任务中，遇到某几个类别区分不开的情况是怎么处理的？

如何根据训练结果判断什么时候是过拟合什么时候是欠拟合？

过拟合的解决方法，优缺点，自己实现了哪些，效果如何？


知道哪些损失函数，优缺点及适用问题？

写一下CrossEntropy公式，并说明一下他的物理意义

F1值计算公式

介绍一下Precision，Recall

AUC原理，为什么更适用于排序问题

介绍一下信息熵，信息增益，信息增益比
信息熵，信息增益，信息增益比各自偏向于什么样的特征

介绍GBDT

XGBoost，Random Forest区别

Random Forest的随机性体现在哪些方面

XGBoost， GBDT区别

XGBoost的原理，有哪些优点

XGBoost子树每个节点是如何分裂的

XGBoost特征选择方式（空间取值的遍历方式）

XGBoost为什么用CART

XGBoost使用细节（特征维度太高的时候，为什么面临输入问题，如何解决）

调用XGBoost使用的个库

介绍一下LightGBM

LightGBM和XGB有什么差异，带来了哪些改进，如何做到的？

是否了解其他的集成模型，介绍一下

决策树分支的标准，gbdt和xgboost的区别

k-means的k值如何确定

相比于直接使用传统分类器，集成学习的方法有哪些优点？

树模型和其他模型之间最大的区别是什么？

为什么树模型不需要对特征进行标准化处理？

为什么一般预测模型要对特征进行标准化处理？

不进行标准化处理会带来哪些问题？为什么？

说说LR 和 SVM的区别

介绍下SVM

SVM损失函数是什么？编程实现一下？

TSVM半监督算法原理及实现细节

是否了解强化学习？简单介绍一下

如何理解多模态？





## 算法题
输出幂集
给出一个无序数组，输出最小的不在数组中的正数。

## 工程情况

## 推荐系统


## 模型部署
## 鸡汤
不要想着一次性学完，每次学一些，积累，总结。

## 方向
问答，mrc（机器阅读理解），对话，匹配，词向量，
迁移，分类，分词，pos，ner

## 模型篇(mrc,问答，分类，ner)
* SGNS/cbow,fast, elmo 词向量
* DSSM, DecAtt, ESIM 问答&匹配
* HAN，DPCNN，分类
* BIDAF，DrQA，QANet MRC
* Cove，InferSent 迁移
* MM， N-shortest 分词
* BI-lstm, crf NER
* LDA等主题模型 文本表示

## 训练篇
* point-wise, pair-wise, list-wise
* 负采样, nce
* 层级softmax，哈夫曼树构建
* 不均衡问题处理
* KL散度，交叉熵函数

## 评价指标
* F1-score
* PPL
* MRR, MAP

## Python
Python里边哈希表对应哪种结构，是如何解决哈希冲突的
Python深浅拷贝
Python中字典的查找时间复杂度