## nlp
说一下Bert嵌入层，Bert相关问题

positional embedding

attention self-attention

### word2vev
word2vec 负采样

word2vec原理，词向量是怎么训练出来的

word2vec和fasttext区别
word2vec和fasttext原理上的区别

Word2vec参数量计算

cbow和skip-gram区别，平时使用哪种方式

介绍一下skip-gram的训练过程，以及使用的加速训练技巧

word2vec有什么优缺点

画一下gru的结构

介绍一下rnn，lstm，transformer各自的优缺点

介绍一下梯度消失，梯度爆炸以及解决方法

rnn为什么容易出现梯度消失、爆炸的原因，能否使用一些技巧缓解

lstm如何解决梯度消失，爆炸的问题

实践中，如何解决梯度爆炸问题，超参数如何设置

设置梯度裁剪阈值时，有没有尝试通过参数分布来计算，而不是直接使用超参数。
### text-cnn, LSTM
介绍一下lstm的原理（h和output的关系）

lstm使用的激活函数是什么？能否使用relu

lstm的参数量，以及初始化方式

有没有见过类似lstm，gru中这种门机制的网络架构

textcnn卷积核大小，为什么要对文本进行卷积，卷积核大小选取标准

textcnn中卷积核的物理意义是什么，提取的是全局特征还是局部特征

textcnn和图像中的cnn最大的区别是什么


### transformer
transformer原理以及自己的理解（相比与之前的方法有哪些优势）

transformer激活函数的位置

transformer的时间复杂度和空间复杂度

transformer的decoder阶段都哪些地方使用了pad

transformer的attention中的softmax中 根号d 代表什么，作用是什么

### bert
讲一讲bert的结构和原理


不考虑多头的原因，self-attention中词向量不乘QKV参数矩阵，会有什么问题？
self-attention的核心是用文本中的其他词来增强目标词的语义表示，从而更好地利用上下文信息。
self-attention中，sequence中的每个词都会和sequence中的每个词做点积去计算相似度。也包括这个词本身

对于self-attention，一般会说它的q=k=v，这里的得相等实际上是指他们来自同一个基础向量，
而在实际计算时，他们是完全不一样的，因为这三者都是乘了QKV参数矩阵的。如果不乘，每个词对应的qkv就是完全一样。

在相同量级的情况下，qi和ki的点积值会是最大的，那在softmax后的加权平均中，该词本身所占的比重将是最大的。
使得其他词

bert为什么要用cls，mlm两种训练方式

bert是如何利用位置信息的（如何训练位置向量），不同方式之间有什么区别

bert相比Word2vec有哪些优势

bert为什么能解决一词多义的问题

bert随机替换掉15%的词，为什么要随机有80%替换为MASK,10%要替换为别的单词
随机10%不替换

DSSM模型和ESIM模型的区别

Bert原理，bert随机mask掉15%的词，为什么要随机有80%替换为[mask],
10%要替换为别的单词，随机10%不替换

文本匹配怎么使用bert

相比于bert得到的各个词向量，直接使用得到的句向量有什么优点

使用bert词向量时，是将文本输入得到output还是直接使用embedding向量，为什么

看过哪些bert改进之后的方法（ALBert，XL-net）

介绍一下ELMo，BERT，GPT之间的区别

self-attention attention multihead-attention mask-attention原理，实现细节

attention如何参考词的位置信息

auc原理（衡量分类器排序的能力），计算（100正，900负，分类器是从0到1均匀分布，计算auc）

样本不平衡如何改进，影响了样本原来的分布该怎么办

代码实现梯度下降

知道哪些模型评价指标

auc和f1的区别，分别使用在什么场景

介绍l1 l2正则化

实现求auc算法，实现lstm

给一个rand5，如何用代码实现rand7
构建[rand5()-1]*5+rand5()
第一项{0, 5, 10, 15, 20},
第二项{1, 2, 3, 4, 5},
加和结果{1, 2, ..., 25}
踢掉大于21的数，剩下的仍是等概率出现的，在对7求余在加1即可得到rand7()

排序算法稳定性，有哪些稳定的排序算法和不稳定的排序算法

了解哪些分类器

了解哪些损失函数

介绍ner，pos

手推logistic regression


自编码

对文本分类最新的方法了解多少？

是否了解label embedding

tfidf原理，公式，弊端是什么


DSSM模型和ESIM模型的区别


## ml
逻辑回归

xgboost

说一下机器学习的pipeline

一般会对原始数据做哪些处理和分析

了解特征工程吗，使用过、知道哪些方法

介绍一个WOE，IV

为什么金融行业使用woe，iv而不是其他特征工程方法

训练集，测试集划分，交叉验证原理

介绍一下训练集，验证集的使用情况，以及实践中遇到的一些问题和技巧。

是否遇到过样本不均衡问题，是如何解决的？

知道哪些解决样本不均衡方法，各自的优缺点，自己实现了哪些，效果如何？

当训练数据和真实数据分布不一样时，线上预测效果会不会出现偏差?

多分类任务中，遇到某几个类别区分不开的情况是怎么处理的？

如何根据训练结果判断什么时候是过拟合什么时候是欠拟合？

过拟合的解决方法，优缺点，自己实现了哪些，效果如何？


知道哪些损失函数，优缺点及适用问题？

写一下CrossEntropy公式，并说明一下他的物理意义

F1值计算公式

介绍一下Precision，Recall

AUC原理，为什么更适用于排序问题

介绍一下信息熵，信息增益，信息增益比
信息熵，信息增益，信息增益比各自偏向于什么样的特征

介绍GBDT

XGBoost，Random Forest区别

Random Forest的随机性体现在哪些方面

XGBoost， GBDT区别

XGBoost的原理，有哪些优点

XGBoost子树每个节点是如何分裂的

XGBoost特征选择方式（空间取值的遍历方式）

XGBoost为什么用CART

XGBoost使用细节（特征维度太高的时候，为什么面临输入问题，如何解决）

调用XGBoost使用的个库

介绍一下LightGBM

LightGBM和XGB有什么差异，带来了哪些改进，如何做到的？

是否了解其他的集成模型，介绍一下

决策树分支的标准，gbdt和xgboost的区别

k-means的k值如何确定

相比于直接使用传统分类器，集成学习的方法有哪些优点？

树模型和其他模型之间最大的区别是什么？

为什么树模型不需要对特征进行标准化处理？

为什么一般预测模型要对特征进行标准化处理？

不进行标准化处理会带来哪些问题？为什么？

说说LR 和 SVM的区别

介绍下SVM

SVM损失函数是什么？编程实现一下？

TSVM半监督算法原理及实现细节

是否了解强化学习？简单介绍一下

如何理解多模态？





## 算法题
输出幂集
给出一个无序数组，输出最小的不在数组中的正数。

## 工程情况

## 推荐系统


## 模型部署
## 鸡汤
不要想着一次性学完，每次学一些，积累，总结。

## 方向
问答，mrc（机器阅读理解），对话，匹配，词向量，
迁移，分类，分词，pos，ner

## 模型篇(mrc,问答，分类，ner)
* SGNS/cbow,fast, elmo 词向量
* DSSM, DecAtt, ESIM 问答&匹配
* HAN，DPCNN，分类
* BIDAF，DrQA，QANet MRC
* Cove，InferSent 迁移
* MM， N-shortest 分词
* BI-lstm, crf NER
* LDA等主题模型 文本表示

## 训练篇
* point-wise, pair-wise, list-wise
* 负采样, nce
* 层级softmax，哈夫曼树构建
* 不均衡问题处理
* KL散度，交叉熵函数

## 评价指标
* F1-score
* PPL
* MRR, MAP

## Python
Python里边哈希表对应哪种结构，是如何解决哈希冲突的
Python深浅拷贝
Python中字典的查找时间复杂度

## 百度实习面经1
### 一轮
1. 先进行自我介绍
2. 介绍自己最熟悉的项目
3. 因为看到你说的这个项目里用到了BERT，那么介绍下Transformer模型
4. Transformer相对于传统的RNN网络有什么好处
5. Transformer里的self-attention作用是什么，有什么优势
6. 你提到了梯度消失的问题，那么Transformer相对于RNN为什么能避免梯度消失
7. 产生梯度消失的原因有哪些
8. 如果你在训练的时候，发现模型不收敛，你会怎么做，可能的原因是什么
9. 学习率是否也会导致模型不收敛
10. 那么学习率应该怎么选择
11. 你提到了优化方法，那么介绍下SGD和ADAM的区别
12. ADAM怎么实现自适应学习率
13. 分类时，样本不均衡问题如何解决
14. 在你的项目中，看到你用CNN，CNN在nlp领域有什么优势和弊端
15. 做题：有序链表合并
16. 最长回文子串
### 二轮
1. 那你就直接介绍一下你最熟悉的一个项目吧
2. 你的项目具体是怎么做的
3. 你的这个数据如何获取，做了什么处理
4. 你的文本大概长度是个什么规模
5. 看你有个文本相似度的项目，你这个项目里用的损失函数是什么
### 三轮
1. 做个自我介绍
2. 介绍一下你最熟悉的项目
3. 你为什么要用你的这个方法
4. 你的这个任务的目标是什么
5. 你对今后从事的工作方向和领域有什么预期，有没有相关的了解或调研？

## 百度健康业务部算法实习
1. tf-idf公式是什么，对于低频词和高频词有处理吗，高频词算出来的tf-idf的值会更大还是更小
2. 特征工程怎么做的
3. xgboost原理
4. w2v原理
5. 为什么会过拟合，怎么解决，归一化的作用
6. 算法题：有序重复数组的查找

## 美团暑期实习NLP工程师面经（技术+HR） 已拿offer
### 一面
1. 简单聊一下工作意向
2. 自我介绍
3. 介绍论文贡献，流程及细节
4. GAN为什么会work
5. vae为什么会work
6. local attention, global attention区别
7. Bert为什么只用Transformer的encoder不用decoder
8. 排队论模型服从什么分布
9. lstm和gru的区别和使用场景
10. 贝叶斯分类的前提假设
11. svm的理论依据，如何推导
12. 编程求2^n

### 二面
1. 介绍文本生成任务
2. 介绍lstm的细节，相关机制，为什么有效
3. 正则化有什么用，为什么有用，L1正则化为什么能使参数稀疏，为什么能防止过拟合。

## 百度暑期实习nlp
### 一面
1. 自我介绍
2. 聊做过的项目，这里注意要按照逻辑顺序交代项目背景，遇到的挑战，手段，成果，
让面试官更清晰的了解你做过的项目。
3. 最大的自序和
4. 树模型，ID3，c4.5区别，RF和GBDT的区别，GBDT是否适合于处理大规模的ID特征
5. 直到哪些激活函数，都有什么特点，如何使用

### 二面
1. 聊做过的项目，和一面讲的是同一个项目。
2. 项目中用过CNN，CNN有什么优势和特点，常用的pooling方法有哪些，哪个更好
3. dssm模型
4. 逻辑回归特征重复了一维会有什么影响

### 三面
1. 聊过做的项目
2. 项目如何验证有效性，在业务中怎么用
3. 聊聊项目中的bad case
4. 训练时，样本不平衡问题如何解决，小样本问题如何解决
5. 给一个任务，垃圾邮件分类，分阶段和流程细致描述一下每一步应该怎么做，以及最终要优化的效果。


